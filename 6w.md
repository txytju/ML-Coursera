# 本周内容
- 算法运行情况监控
- 算法调试
- 机器学习程序设计
- 算法优化方法

# 哪些动作可能改进算法&可能对算法没有改进
- 有哪些算法？
  - 获得更多的数据很可能不会有改变
  - 使用更少的特征
  - 添加新的更多的特征
  - 使用更高阶多项式
  - 增加或者减小正则化参数 $\lambda$
- 不应该随机（鲁莽）的选择某一种方法随算法进行改进，而是应该使用诊断法来测试算法的性能。

# 评估机器学习算法的性能
- 使用训练集和测试集对算法的训练结果进行评估
  - 对于线性回归：可以使用测试集上的损失函数计算值和训练集上的损失函数计算值进行对比
  - 对于逻辑回归：可以计算分类错误率
$err = 1 $，$if h_{\theta}{(x)} < 0.5,y = 1$  $or$ $if h_{\theta}{(x)}>0.5,y=0$

# High Bias vs. High Variance
- High Bias（欠拟合）。在这种情况下，训练集和测试集上的误差都很大，在一个数量级上（约等于级别）。
- High Variance（过拟合）。在这种情况下，训练集上的误差很小，但是测试集上的误差很大（几倍以上）。
- 多项式的阶数的影响（假设函数复杂程度对欠拟合、过拟合的影响）
![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/TqClMD_pEee3MRIl4lCYSA_abe0ea29a68dbe65f530289ee942d9b3_fixed.png?expiry=1499212800000&hmac=hVoJ0QZsJh3KIrCVF0mqVTkDyldARnhfgJhCPh-fEBc)
- 正则化系数对过拟合、欠拟合的影响
  - 随着正则化系数越来越大，训练集上的误差越来越大；验证集上的误差先减小后增大，有一个合适的正则化参数的取值区间。

# learning curve
- 什么是`learning curve`
  - 横轴是训练样本的数量
  - 纵轴是`error`
  - 有两条曲线，分别代表训练集上和验证集上的`error`
- 如何绘制`learning curve`

  ```python
  for 样本数量 = 1 ：m
      1. 使用训练集训练参数`theta`
        - 在某一个确定的正则化系数的参与下，使用训练集训练参数`theta`。这个过程就是我们所熟悉的训练模型的过程。
      2. 计算在这个特定的参数`theta`下、并且不使用正则化参数（即`lambda = 0`）时在上述训练集和验证集中的`error`，分别得到`error_train(i)`和`error_val(i)`。
  end
  - 绘制`i, error_train, error_val`的关系曲线。

  ```
- 有什么用途
  - `learning curve`常用来判断一个假设函数对数据的是过拟合还是欠拟合
    - 若两个曲线最终接近并且都很大，说明模型欠拟合
    - 若两个曲线差别较大（训练集上小，验证集上大），说明模型过拟合。

# 挑选合适的正则化参数

# 识别垃圾邮件
- 可能改进垃圾邮件分类的方法
  - 获得更多的数据
  - 特征向量：由很多个词组成的一个特征向量，在一封邮件中，如果对应的词出现，则向量对应位置的数值为1，否则为0。特征向量是描述一封邮件的重要特征。词向量的长度约50000~80000（通过在训练集中筛选最常出现的词），因此一般是稀疏矩阵，在这种情况下更多的数据是有帮助的。
  - 根据邮件的发送来源做判断（服务器路由等）
  - 单词大小写，标点符号（如感叹号）的数量等
  - 对邮件内容进行拼写检查并纠错。垃圾邮件的发送者为避免其文中的词被识别，常常故意将一些词拼错，以逃避特征向量的检查。自动纠错有助于提升识别准确率。
- 既然有可能有这么多方面影响模型的能力，那么当拿到一个机器学习问题时，首先应该做什么？
  - 头脑风暴，想想可能影响模型的因素有哪些？要考虑哪些因素？先思考，再动手。

# 错误分析（`error analysis`）
- 当已经实现了一版比较简单的模型版本（MVP）之后，或者针对一个较为完善的模型算法，一个找到提升模型能力的好方法是误差分析。误差分析是指，在**验证集**上测试模型，然后观察验证集上的错误分类（如果是一个分类器），尝试观察错误分类遵循什么样的`pattern`，以相应的找到改进模型的反向或者好的优化算法。
- 错误分析中暗含了最小行动的思想，或者从错误中学习的思想。在进行模型的最初设计时，不应考虑太多的复杂因素（容易引起模型的过早的无谓的优化），而是应该在设计简单模型之后测试它，从错误中学习。
- 推荐的解决机器学习问题的思路
  1. 简单模型开始，应用，并使用验证集测试
  2. 画`learning curve`，以观察模型是过拟合还是欠拟合，并寻求相应的解决策略（增加训练样本，增加减少特征等等）。
  3. `error analysis`。寻找改进模型的可能`pattern`。

# 数值评价（`Numerical evaluation`）
- 一种方法或者改进好用不好用，用数据说话。使用不同方法并在验证集上测试效果，选较好的方法。

# 偏分类（`Skewed Data`）和评价模型的标准（准确率与召回率）
- 什么是偏分类
  - 在一个分类问题中，如果其中的某一类的比例显著多于另外一类，称为偏分类。典型的情况如癌症诊断，得癌症（`y=1`）的比例要远远低于不得癌症（`y=0`）的比例。
- 偏分类问题有什么影响
  - 偏分类问题使得我们原有的用分类准确率来描述算法结果的方式不再适用。举个例子，如果癌症患者比例0.5%，那么如果一个模型是`y=0`，这个模型的分类准确率是99.5%，表面上看起来很高。实际上我们却被算法“欺骗”了。为了准确描述这种偏分类情形下模型的能力，应该使用`percision`和`recall`。
- 什么是精确率（`percision`）和召回率（`recall`）
  - `percision = True posotive / # predicted positive = True positive / (True positive + Flase positive)`
  - `recall = True positive / actural positive = True positive / (Ture positive + False negetive)`
- 精确率和召回率，如何权衡？
  - 精确率和召回率两者不可兼得
  - 对于一个分类问题，精确率和召回率的大小取决于使用的分类阈值（`therehold`），如果使用的阈值高，比如0.7、0.9，意味着只有当非常确信`y=1`时才会判定为1，这样精确率高，召回率低；反之召回率高，精确率低。
  - 两者的取舍很大程度上取决于在一个具体的问题中两者那个更重要。
- `F1 Score`
  - 一般来说，我们喜欢用一个指标判定一个模型到底是好还是不好，比如分类准确率，但是在偏分类问题中，由于分类准确率的无效性，我们有了两个精确率和召回率两个参数，这两个参数虽然可以更清楚的描述模型结果，但是却失去了一个统一的判断模型好坏的指标。
  - 基于这种情形，可以定义一个参数，来衡量模型，如`F1 Score`。当两者有任何一者为0时，得分为0，当两者都是1时（模型完美），得分为1。
  $$F1 = 2\frac{FR}{F+R}$$

# 大数据有用吗？
- 一种说法：提高模型性能的更多的是数据，而不是使用哪种算法。
- 在什么情况下，使用**一种特定的算法**，并使用**大量的数据**，确实会提升模型的性能？
  - **特征（`Features`）里有预测`y`的充足的信息。**（Assume feature x (R(n+1)) has sufficient information to predict y accurately.）。即特征的充分性，如果特征选的就不充分，那数据再多也没用。比如如果只选一个特征（房子的面积）来预测房屋价格，那么无论多少数据都是没用的。相反的例子是计算机视觉，一张黑白图片的所有信息就是像素点上的灰度，这是完全信息的，因此特征加上足够多的数据可以训练一个很精确的模型。
  - 关于一个问题是不是可以用机器学习问题解决的测试：**在有输入条件的情况下，一个人类专家可以有信息预测`y`吗？**`(Given the input x, can a human expert confidently predict y ?)`
- `Solution to high bias and high variance problem`：足够多的特征 + 足够多的数据
  - 特征解决了模型的`high bias `问题，即模型不会欠拟合
  - 数据解决了模型的`high variance `问题，即模型不会过拟合

# Chap1
## 有监督学习与无监督学习
- 应用哪一种、如何应用机器学习算法更重要
- 有监督学习：有正确答案的学习
  - 回归问题：目标值连续
  - 分类问题：目标值不连续（多个备选的不连续值）
  - Feature：学习算法的输入
  - 特征数量无数：SVM，可以帮助机器处理无数的Fearture，`a trick`
- 无监督学习：没有正确答案的学习
  - Idea : Here is the data set, can you find the structure of the data set?
  - 聚类（`Clustering`）
    - 例子
      - Google News，同一主题的相关新闻聚在一起
      - 根据人的很多基因的是否表达，将很多个体分为不同的种类
      - 大型计算机集群的协同工作，聚类算法使得协同工作更加有效
      - 基于社交网络的数据将人分为不同的群组，进行社交网络分析
      - 根据公司提供的客户数据，将用户分为不同的细分市场
      - 天文数据分析，星系如何诞生
  - 非聚类问题（`Non-Clustering`）
    - 例子：鸡尾酒问题
      - 两种声音叠加，使用算法进行区分
## 损失函数与梯度下降
- 损失函数
  - Idea：Choose parameters so that predicted function $h(x)$ is close to $y$ for train examples
  - `平方误差损失函数（Square error function）` 是回归问题中常用的损失函数

- 梯度下降
  - 目的是：有一个损失函数，找损失函数的最小值以及其对应的参数
  - 梯度下降的一般表达式：设损失函数与$n$个参数相关
  $$\theta_{j} = \theta_{j} - \alpha\frac{\partial}{\partial{\theta_{j}}}J(\theta)$$
  其中，$\theta = (\theta_{0}, \theta_{1}...\theta_{n})$
  - 要点：不同的参数要`同时更新（Simultaneous Update）`
    - 正确的做法：不同的参数要从相同的起点出发，同时迈步子
  $temp0 := \theta_{0} - \alpha\frac{\partial}{\partial{\theta_{0}}}J(\theta)$
  $temp1 := \theta_{1} - \alpha\frac{\partial}{\partial{\theta_{1}}}J(\theta)$
  $\theta_{0}:=temp0$
  $\theta_{1}:=temp1$
    - 错误的做法：一个参数已经更新过了，另一个参数才更新
  $temp0 := \theta_{0} - \alpha\frac{\partial}{\partial{\theta_{0}}}J(\theta)$
  $\theta_{0}:=temp0$
  $temp1 := \theta_{1} - \alpha\frac{\partial}{\partial{\theta_{1}}}J(\theta)$
    $\theta_{1}:=temp1$


- 梯度下降的学习率
  - 学习率设置对学习过程的可能影响
    - 过小的学习率导致学习速度过慢
    - 过大的学习率可能导致收敛失败甚至发散
  - 学习率（$\alpha$）即便固定（fixed），但是当其设置适当时（不过大），参数$\theta$可以收敛到极小值。
    - 这是因为虽然学习率（$\alpha$）固定，但是随着$\theta$越接近其极小值，$\frac{\partial}{\partial{\theta_{j}}}J(\theta)$也在逐渐变小。
    - 因此，随着训练减小学习率（$\alpha$）并不是必要的。
  $$\theta_{j} = \theta_{j} - \alpha\frac{\partial}{\partial{\theta_{j}}}J(\theta)$$

- 使用梯度下降算法，最小化线性回归的损失函数
  - 将损失函数的表达式带入到上述梯度下降的表达式中，得到使用梯度下降算法计算线性回归参数的表达式
$$\theta_{0} := \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})$$
$$\theta_{1} := \theta_{1} - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)}) \cdot x^{(i)}$$
  - 凸函数（Convex Function），只有一个极值点

# Chap2
## 多变量线性回归
- 变量命名约定
  - 训练样本的数量：$m$
  - 特征的数量：$n$
  - 第$i$个训练样本：$x^{(i)}$
  - 第$i$个训练样本的第$j$个特征：$x_{j}^{(i)}$
- 目标函数
  - $h_{\theta}(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}$
  - 假设$x_{0}=1$
  - 上述表达式可以表示为$h_{\theta}(x)=\theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}=\theta^{T}X$
## 多变量的梯度下降
- 多变量线性回归的梯度下降计算表达式为
$$\theta_{j} := \theta_{j} - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)}) \cdot x_{j}^{(i)}$$

## 特征缩放和均值归一化
- 目的：梯度下降算法收敛更快。

- 特征缩放（Feature Scaling）

  - 为什么要进行特征缩放？
    - 仅以两个特征$x_{1},x_{2}$的情况进行比较，因为方便可视化。
    - 当两个特征的取值范围接近时，其损失函数的图像更像是一个圆形的碗，这会导致这样一种后果：在任意一个点处取平面向量$(\frac{\partial{J}}{\partial{x_{1}}}, \frac{\partial{J}}{\partial{x_{2}}})$，都会更接近指向于碗的中央，也就是梯度下降的全局最优解。
    - 如果两个特征$x_{1},x_{2}$在数值上相差巨大，那么对于多变量线性回归而言，其损失函数的针对这两个特征的图像是一个扁平的“峡谷”，在任意一个点处取平面向量$(\frac{\partial{J}}{\partial{x_{1}}}, \frac{\partial{J}}{\partial{x_{2}}})$，由于峡谷太过于狭长（极限情况下可以想像成一个方向无限长），那么这个向量会极其偏向于指向其中的一个方向（垂直于狭长峡谷面的方向），进而完全偏离了指向峡谷谷底的方向，虽然最终也能收敛于谷底，但是在过程中会多走很多锯齿状的弯路，效率低。
  - 常用的特征缩放是将多有的特征取值范围均缩放到$-1<x_{j}^{(i)}<1$之间。
    - 首先，如果存在$\theta_{0}$，那么对应的$x_{0}$就是1，所以其他的变量应该以其为基准进行特征缩放。
    - 这个取值范围并不严格，也就是说，如果某个特征的取值范围在$(-1,1)$这个数量级附近，如$(-2,0)$可不做调整；
    - 只是当数量级出现偏差，如$(-100,100)$，$(-0.001,0.001)$这种情况时才需要进行缩放。
    - $Andrew NG$的经验：$(-3,3)$以及$(-\frac{1}{3},\frac{1}{3})$可以不做调整


- 均值归一化（Mean Normalization）
  - 为什么要进行均值归一化
    - ？
  - 常用的均值归一化的方式
    - 将原来的$x_{j}$改成 $x_{j}-\mu_{j}$ ，其中$\mu_{j}$是数据样本中第$j$个特征的取值的平均值



- 特征缩放+均值归一化
$$x_{j} \rightarrow  \frac{x_{j}-\mu_{j}}{S_{j}}$$
其中${S_{j}}$可以是$x_{j}$数据样本中最大值与最小值之间的差值，或者样本标准差
- 在神经网络的训练中，特征缩放和均值归一化非常重要

## 学习率
- 损失函数与迭代次数的关系：如果学习率设置的足够小，随着迭代次数的增加，损失函数应该越来越小
- 用图像来判断是否已经收敛是重要的
- 如果损失函数随着迭代越来越大，或者损失函数与迭代次数之间是波浪式的关系，最可能说明学习率设置的太大了
- 找到合适学习率的方式：不断尝试，可以是3倍的关系尝试：$0.001,0.003,0.01,0.03,0.1,0.3,1$...

## 特征选取与多项式回归（polynomial regression）
- 特征的选取
  - 特征的选取可以有多种形式
    - 一些算法会自动选取重要的特征
    - 一些算法需要人为指定特征，在这种情况下，对特征的洞见可以帮助建立更好的学习模型
- 多项式回归
  - 多项式回归是众多的可能形式之一
- 特征缩放
  - 当使用多种特征时，特征缩放是重要的考虑内容

## 正则方程（Normal Equation）：求解线性回归的解析方法
$$\theta = (X^{T}X)^{-1}X^{T}y$$
- 不需要做特征缩放
- 梯度下降和正则方程的特点
  - 梯度下降
    - 需要设置学习率
    - 需要迭代
    - $O(kn^{2})$
    - 当特征数大时计算效率高
  - 正则方程
    - 不需要设置学习率
    - 不需要迭代
    - $O(n^{3})$，需要计算 $X^{T}X$
    - 当特征数大时计算效率低
  - 实际中，当特征数超过10,000时，一般使用梯度下降

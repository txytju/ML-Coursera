# 在大训练样本的情况下使用梯度下降算法
- 随着数据（尤其是网络数据）的爆炸式增长，在机器学习问题中得到超大数量级的训练样本成为可能。相应的，由于对大量数据进行数据处理时需要消耗大量的计算资源，因此需要相匹配的训练技巧及算法。

- 首先需要问的问题是：真的需要用到这么多训练数据吗？
  - 当拿到一个机器学习问题时，首先需要做的不是直接对大量数据进行训练，而是启发式的观察模型到底属于什么样的模式？
  - 对于面临的机器学习问题，首先使用小数据集对我们的模型做一个测试。如果发现模型是过拟合的，则可以使用更多训练样本对模型进行训练；如果模型已经欠拟合了，则使用多少数据都没用，我们首要考虑的是模型本身是不是对数据的合理描述？

- 随机梯度下降
  - 概念：与使用全部训练样本进行每一步训练不同，在每个训练步中只使用1个训练样本进行训练，称为随即梯度下降。
  - 随机梯度下降的步骤：
    1. 打乱训练样本的顺序
    2. 循环，每次选一个训练样本进行训练
  - 对整个训练样本进行 1-10次训练，即最少使用全部训练数据训练一遍就可以，同时也可对全部训练数据遍历多次（少于10次）
  - 如何判断随即梯度下降是否在收敛呢？
    1. 在每一步中，使用该训练样本更新参数前，计算当前模型（未更新参数的模型）在该训练样本上的cost
    2. 每隔一定的步数，计算过往一定数量的cost的平均值，观察cost随着训练的进行的变化。


- 小批量随机梯度下降
  - 设定一个小批量数据的size，每次对一个size的数据进行计算之后更新模型。
  - 小批量相比随机梯度下降的优势：体现在向量化上。当小批量随机梯度下降算法应用了较好的向量化计算方法时，可以在一定程度上利用并行计算（并行计算b个样本），因此其在计算效率上相比随机梯度下降有优势。
  - size参数b需要进行调试（超参数），参数b的范围在(2,100)都是可以的。

# Online Learning
- 使用新数据对已有模型进行微调，当使用过该训练数据后，舍弃该训练数据
- Online Learning 的好处
  - 对于一些需要时刻跟进用于偏好变化的机器学习模型（如推荐系统）来说，Online Learning可以保持模型的更新，跟上用户偏好的变化。

# 并行计算和 Map Reduce
- 并行计算
  - 为什么需要并行计算
    - 在进行机器学习时，在算法中常常涉及大量的数学运算。如果训练样本数量非常大，那么训练过程中会使用大量的计算资源，同时消耗大量的计算时间。如果可以将上述计算任务分发到不同的计算机上，由多个计算机同时完成，就可以提升程序运行的效率。这就是并行计算的简单原理。
  - 什么条件下可以进行并行计算
    - 对于机器学习算法，如果在计算过程中的一些量可以表示为`在整个训练数据集上求和的形式`，那么就可以考虑使用并行计算的方式，将这个大的求和拆分成若干个小的求和，分配到不同计算资源分别计算，计算完成后再将计算结果统一整合到一个机器上（或者服务器上）。
    - 对于大多数机器学习算法，其损失函数中都有求和，因此都可以使用并行计算。
  - 并行计算的简单原理
    - 将计算拆分成多个部分，经过`传输-计算-传输-加和`的方式实现并行计算。
  - 影响并行计算效率的因素

- Map Reduce
  -

- Hadoop

- 单个计算机处理器的多核
  - 为什么计算机的处理器有多个核？

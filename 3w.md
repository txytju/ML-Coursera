## 分类问题
- 例子
  - 垃圾邮件
  - 交易欺诈
  - 肿瘤：良性恶性
- 一般
  - $y=0$：`negatice class`（找的事情没发生）
  - $y=1$：`positive class`（找的事情发生了）
- 逻辑回归
  - $0 \le h_{\theta}(x)\le 1$
- 函数的解释
  - 用于分类问题的函数：sigmoid函数
$$h(x) = \frac{1}{e^{-(\theta^{T}*x)}}$$
  - 函数的意义：$h_{\theta}{(x)}$是指，在给定的`Feature条件（已有的X）`下，基于参数 $\theta$，`positive class`发生的概率，即
$$h_{\theta}{(x)} = P(y=1|x;\theta)$$
  - 由于是分类问题，因此给定一个界限值`therehold`，如0.5
    - 当$h_{\theta}{(x)}\ge0.5$，时，推断 $y=1$，可推导出等价条件$\theta*x \ge 0$
    - 当$h_{\theta}{(x)}<0.5$，时，推断 $y=0$，可推导出等价条件$\theta*x<0$
- 线性决策界面
  - 一元一次（$x_{1}$）的决策界面是一条垂直于$X$轴的竖线
  - 二元一次$(x_{1},x_{2})$的决策界面是$(x_{1},x_{2})$平面内的一条直线
  - 多元一次$(x_{1},x_{2}..x_{n})$的是一个平面/超平面
- 非线性决策界面

## 逻辑回归模型
- 由于sigmoid函数的复杂性，如果沿用线性回归的损失函数，那么损失函数对 $\theta$ 的图形是非凸的。
- 逻辑回归的损失函数怎么定义？
  - 基本条件
    - 连续的
    - 始终大于等于0
    - 满足损失函数的意义
      - 当$h_{\theta}{(x)}=1$并且$y=1$时，损失函数应为0；
      - 当$h_{\theta}{(x)}=0$并且$y=0$时，损失函数应为0；
    - 是一个凸函数
    - 逻辑回归的损失函数
      - 当$y=1$时，
$$Cost Function = -log(h_{\theta}{(x)}) $$
      - 当$y=0$时，
$$Cost Function = -log(1-h_{\theta}{(x)})$$
- 上述损失函数写在一起，就是交叉熵损失函数
$$Cost Function = -y * log(h_{\theta}{(x)}) -(1-y) * log(1-h_{\theta}{(x)})$$
$$J(\theta) = \frac{-1}{m}\sum_{i=1}^{m}[y^{(i)}log(h_{\theta}{(x^{(i)})+(1-y^{(i)})}log(1-h_{\theta}{(x^{(i)})})]$$
  - 这个损失函数是凸函数
- 使用梯度下降算法求 $\theta$，这个损失函数对$\theta$的偏导数的计算化简结果，和线性回归中使用二次函数作为损失函数计算得到的化简结果完全相同，如下。其推导过程见[交叉熵代价函数+方向导数+梯度（详细作用及公式推导）](http://blog.csdn.net/u013527419/article/details/60322106)
$$\theta := \theta - \frac{\alpha}{m}[\sum_{i=1}^{m}[h_{\theta}{(x^{(i)}-y^{(i)})}]* x^{(i)}]$$
- 交叉熵做损失函数的优势
  - 由上述计算梯度下降的公式可以知道，参数更新的斜率（快慢）和 $h_{\theta}{x^{(i)}-y^{(i)}}$ 成正相关，也就是说，错得越离谱，学习的越快。
- 交叉熵损失函数是如何推导来的？
- 上述梯度下降的向量化实现？
$$$$

- 优化算法
  - 在算法中，要计算两个对象
    - 损失函数
    - 损失函数对参数的偏导数
  - 除了梯度下降之外，还有其他优化算法
    - Conjugeta gradient
    - BFGS
    - L-BFGS
  - 相比梯度下降算法，上述算法的优点
    - 不需手动设置学习率（现行搜索算法，为不同迭代选择不同学习率）
    - 收敛快
    - 但是上述算法都比梯度下降算法复杂得多
  - 使用 high-level方法去实现这些复杂算法

## 多分类问题（One Vers All）
1. 将多分类问题转化成多个二分类问题。也就是说，有多少种分类（$n\ge3$），就进行多少次的二分类问题，在第$i$个二分类问题中，将第$i$类看成是`想要的对象`，其他的看成是不想要的对象，因此原来的假设函数的意义变为
$$h_{\theta}(x)=P(y=i|x,\theta)$$
2. 利用上述二分类问题的计算方法，得到 $n$ 种 $\theta$ （$\theta$是向量），也就是得到了$n$个由数据计算而来的假设函数
3. 那么对应一个新的数据点，将其代入上述$n$个假设函数中，得到的可能性最大的类就是它的分类
$$is \quad class \quad i \quad where \quad max [h_{\theta(i)}(x)]$$

## 过拟合问题
### 1. 什么是过拟合问题
过拟合，泛化能力差。泛化能力是指一个经过训练的样本模拟新样本的能力。
- 这里的“经过训练”也有两方面的含义
  - 一方面，当经过非常多次的迭代，我们判定模型已经稳定的时候，我们称模型“已经经过训练了”。
  - 另外一方面，训练具有时间维度。也就是说，一个模型，并不一定就是像第一条说的是训练到稳定的结果，如果在训练的过程中停下来，这个模型也可以称为“经过训练的”，极端条件下，如果只迭代一次，我们也必须称这个模型“已经经过训练了”。
  - 上边两个方面的观点可以总结为一句话：一个经过训练的模型是指，一个人为假定的模型（超参数），在喂给数据之后，经过一定数量的训练（不一定达到稳定状态），得到的状态。

### 2. 什么时候容易出现过拟合问题
- 出现过拟合问题的情景
    - 过拟合的情况在这种情景发生：一个模型，其`特征数`相比`喂给它的训练样本的数量`，是`相对多`的，那么在这种情况下，如果不能在训练迭代时`在适当的时候`停下来，仍然继续训练，那么得到的模型是过拟合的。
    - 拆分开来就是：
      1. 模型特征过多；
      2. 在特征多的情况下还不停训练。
    - 上述两个条件是有先后的，也就是说：
      - 如果特征选的合适，那一路训练到底也不会过拟合。
      - 如果特征选择的过多了，那只要在合适的时候停止训练，也可以算得上是个没过拟合的模型。
- 解释几个重点词
  - 相对多：相对多的含义，并不是数学上的`特征数`>`喂给它的训练样本的数量`，而是描述了这样一种状态：在给定一些特征时，模型过分地契合训练样本。同时，`特征数`也不单纯是一个数字，表述数字上有几个特征，而是选了`哪些特征，选的多不多？`，抓到数据`pattern`的特征和没抓到数据`pattern`，不具有同等的地位。
  - 适当的时候
    - 以分类问题举个例子。在训练模型的过程中，观察这么几个指标：训练样本识别准确率、验证样本识别准确率、训练集上的cost，验证集上的cost，其中前两个是重点。
      1. 随着训练的过程，模型对训练样本的识别率会逐渐升高，因为模型在“学习”这些训练数据，同时在开始时候验证集中的识别率也会升高。
      2. 到某一时刻开始，验证集上的识别率开始降低，而训练集上的识别率继续升高，那么在这个转折点处，应该停止。因为在继续训练下去，模型只会去“机械记忆”训练数据，而没有了泛化的能力。

### 3. 如何防止模型过拟合（`Addressing Overfitting`）
0. 增加样本的数量
1. 选取合适的特征及其数量（特征工程）
    - 方法
      - 人工选择哪些特征是重要的，予以保留
      - 使用一些算法来选择特征
    - 减少特征数有一定的局限性，因为减少特征意味着减少信息
2. 正则化（`Regularization`）
    - 保留所有的特征，但是将特征的权重$\theta_{j}$减小
    - 这种方法适用于有很多种特征，没有特别明显的谁是突出谁是次要，也就是都对结果有一定的影响的情况。
3. 早停（`Early Stopping`）
    - 在训练迭代到合适情况下及时停止。

#### 4.正则化（`Regularization`）
- 带有惩罚项的损失函数
  - 如果损失函数是误差平方和的话，则相应的带有惩罚项的损失函数是
$$J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}(h(\theta^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_{j}^{2}]$$
  - 其中的$\lambda$称为正则化参数（`regularization parameter`），当它强调了以下两种状态之间的平衡
    - `假设函数`可以很好的描述训练数据
    - `假设函数`不过拟合（使$\theta$尽量小一些）
  - 当$\lambda$越大时，表达了控制不过拟合的想法越强烈；相反，则是希望假设函数可以更好地描述数据的想法更强烈。极端情况下，$\lambda$非常大，则$\theta$都有趋于0的趋势。


- 疑问：在上述惩罚函数中，由于不同$\theta$的系数都相同，都是1，是不是就相当于把这些系数等同看待了呢？如果在一些情景中，一些参数明显比另外一些参数重要（比如预测房价的假设中，一次二次的系数明显高于三次四次），那么如何体现这种不同参数之间不同的重要程度？上述罚函数可以体现吗？
  - 回答：不同$\theta$的系数的重要性并不相同，这种不同体现在`其让原始损失函数减小的能力上`。
    - 举个例子，有两个参数 $\theta_{1}$ 和 $\theta_{2}$，那么由于有乘法项的存在，两者都有绝对值变小的趋势。但是在这种情况下，如果$\theta_{1}$绝对值变小会引起原始损失函数的急剧变大，而$\theta_{2}$绝对值变小时，损失函数几乎没什么反应，那么在这种情况下，最后的迭代结果必然是$\theta_{1}$没有多少变化，而$\theta_{2}$变得非常小了。形象的说法就是，谁能力强，谁就保留；谁能力不行，就被削弱。


- 当使用上述损失函数时，对应的梯度下降算法的更新式为：（对$\theta_{0}$不使用正则化）
  $$\theta_{0} = \theta_{0} - \alpha\frac{1}{m}\sum_{i=1}^{m}(h(\theta^{(i)})-y^{(i)})x_{0}^{(i)}$$
  和$$\theta_{j} = \theta_{j} - \alpha \frac{1}{m}[\sum_{i=1}^{m}(h(\theta^{(i)})-y^{(i)})x_{j}^{(i)} + \lambda \theta_{j}]$$
  即
  $$\theta_{j} = \theta_{j}(1-\alpha \frac{\lambda}{m}) - \alpha \frac{1}{m}[\sum_{i=1}^{m}(h(\theta^{(i)})-y^{(i)})x_{j}^{(i)}]$$
  - 上述式子能保证$\theta$有更小的绝对值吗？







##



























#
[最小二乘、极大似然、梯度下降有何区别？ - 知乎](https://www.zhihu.com/question/24900876)

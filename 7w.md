- 什么叫核函数？为什么将其称之为核函数？
  - 首先，核函数和映射没有关系，核函数只是用来计算映射到高维空间之后的内积的一种简便方法。展开一点说，当特征或者数据被从一个小的空间映射到一个高维度空间时，如果要计算某些对象的内积会耗费比较多的计算资源，因此核函数提供了一个用原始维度对象计算高维度内积的`trick`。
  - 核函数的公式
$K(x,y) = <f(x), f(y)>$
    - 其中 $x$ 一个低维度空间（m维）中的向量，$f(x)$ 是一个n维空间中的向量，这个操作是讲一个m的向量映射到n维空间。$<>$是内积，就是向量的点积。
    - 可以证明的是，对于一种特定的映射方式 $f(x)$，都有一个特定的核函数 $K(x,y)$ 与之对应。当然也只有在这种对应关系下，上述的`核函数等于内积`的等式关系才能成立。


- 什么是SVM
  - SVM本质上是一个线性分类器，并且是以`大间距`为目标的线性分类器。当被分类对象不满足线性可分条件时，将其从低维度映射到高维度以满足线性可分的条件。即，SVM就是 $y = w' * \varphi(x) + b$，其中$\varphi(x)$是特征向量（feature vactors），并且是$\varphi(x)$使得数据从低维投射到高维空间后实现了线性可分。

- 核函数与SVM之间有什么关系？
  - 核函数与SVM两者是相互独立的，或者说是正交的概念。
  - kernel是在SVM解对偶问题的最优化问题时，使得$\varphi(x)$能够更方便的计算出来，尤其是在其维度很高的时候。



- 摩塞尔定理




- 对于使用SVM算法的建议
  - 使用现有的成熟的SVM工具
  - 选择参数与核函数
    - 选择参数`C`
    - 选择使用哪一种核函数
      - 线性核函数
        - 什么时候使用线性核函数：特征数量大，训练样本少的时候，相当于是在一个高维空间中的一个稀疏的样本，建议使用线性核。
      - 高斯核函数
        - 除选择`C`之外，还要选择 $\sigma$
        - 什么时候使用高斯核函数：需要复杂非线性分类器的时候。如果特征数量少，比如就两个特征，但是训练样本多，就需要一个在低维度空间中的一个复杂的非线性分类器，此时高斯核合适。


- 使用高斯核函数时，参数 $C$ 和 $\sigma$ 如何影响模型的倾向性：`high bias or high variance`
  - $C$
    - 当$C$很大时，损失函数的特征要求分类器的分类结果非常严格（一个也不能错，或者基本不能错），这导致了如果有离群值存在，分类界面会严重地受离群值的影响，也就是说会产生`high variance`问题。
    - 相反的，当 $C$ 小时，上述要求没那么严格，模型可以容忍一定的错误分类结果，模型对离群值的鲁棒性更强，属于`high bias`问题。
  - $\sigma$
    - 如果$\sigma$很大，当x改变时也不怎么影响输出，是`high bias`
    - 如果很小，x的微小改变会影响输出，是`high variance`
  - 如何选择合适的`C`和 $\sigma$
    - 与其他的方法相似，在SVM中，上述两个参数是SVM模型的超参数，需要通过模型在验证集上的表现来选择合适的超参数。

- 使用高斯核函数时，应首先对参数进行归一化
  - 因为在计算两个点（两个样本）之间的高维度空间距离（2范数）时，数值大的特征会对计算结果产生相对大的而影响，这种大的影响对其他的特征是不公平的。

- 多分类问题
  - 使用现有的包中的多分类方法
  - 使用`one vs all`

- 什么时候使用逻辑回归或者SVM
  - 和样本数量相比，特征数量非常大（高维空间中的稀疏点）
    - 逻辑回归或者没有核函数的SVM足以满足
  - 小特征集，中训练样本（n = 1~1000, m < 10000）
    - 使用高斯核函数的SVM，并且训练速度可以接受
  - 小特征集，大训练样本（n = 1~1000, m = 50000+）
    - 如果使用高斯核函数的SVM则训练速度不可接受。一般情况下人为创建多一些特征，然后使用逻辑回归或者没有核函数的SVM
  - 值得一提的是，上述三种情形中其实都可以使用神经网络来训练，只是神经网络的训练速度要比一个逻辑回归或者SVM的训练速度慢的多
- SVM的优化问题是凸优化问题，不用担心局部最优的问题


- [SVM 的优点与缺点](http://scikit-learn.org/stable/modules/svm.html#svm)
  - 优点
    - Effective in high dimensional spaces.
    - Still effective in cases where number of dimensions is greater than the number of samples.
    - Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.
    - Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.
  - 缺点
    - If the number of features is much greater than the number of samples, the method is likely to give poor performances.
    - SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.

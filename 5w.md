# 为什么需要将矩阵转换成向量形式
- 矩阵形式和向量形式的优点
  - 矩阵形式
    - 在进行正向传播和反向传播的过程中，矩阵的表达实现了向量化，方便向量和矩阵的计算， 实现程序非常简介
  - 向量形式
    - 在进行一些高效的数值算法计算时，算法常常要求输出的参数是向量的形式，因此需要将矩阵转换成向量输入这些算法，并在结束计算之后将向量重新转换成矩阵。
```matlab
thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ]
Theta1 = reshape(thetaVector(1:110),10,11)
Theta2 = reshape(thetaVector(111:220),10,11)
Theta3 = reshape(thetaVector(221:231),1,11)
```

# 梯度检查
- 梯度检查的内容：使用梯度的原始定义来计算一下梯度，并和反向传播算法的计算结果做个比较，用来做校核。
- 为什么使用反向传播算法而不是直接使用定义法计算偏导数：因为直接计算法相比反向传播太慢，只适合用来做校核。
- 常见流程
  1. 使用反向传播计算偏导数
  2. 使用定义法计算偏导并比较（梯度检查）
  3. 若两者接近，关闭梯度检查并使用反向传播算法进行训练
```matlab
epsilon = 1e-4;
thetaPlus = theta;
thetaMinus = theta;
for i = 1:n,
    thetaPlus(i) += epsilon;
    thetaMinus(i) -= epsilon;
    gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)
end;
```

# 随机初始化
- 为什么要做初始变量的随机？初始变量全是零或者其他某一个固定的数值行不行？
  - 下一层神经元的输出值由两层之间的权重矩阵决定，当
    1. 由上一层神经元的一个个体到下一层神经元的所有个体之间的权重都相等
    2. 所有的上一层神经元的个体都满足条件1，时
  - 下一层神经元的每个个体实际上是同质化的，因为其表达式完全相同，其所代表的特征也完全相同，同时继而在后续的学习中（反向传播）其能学到的特征也完全一样（像一层只有一个神经元一样），神经网络失去了意义。
- 可以使用随机分布的形式进行初始化，如一系列均值是0，在一定范围内的正态分布。


# 如何训练神经网络
1. 选择神经网络的结构，包括输入输出神经元的个数，隐层的层数和每层的数量
   - 输入神经元个数：特征数量
   - 输出神经元个数：如果是分类问题，就是可能的分类数量
   - 隐层数与每层神经元数量：可以从1层开始尝试。也可以使用多层，并默认每层使用相同数量的神经元。
   - 每层神经元的数量：该数量要与特征数相匹配，可以是特征数的多倍，一般来说隐层神经元数量多于输入神经元的数量时就是可行的。
2. 权重初始化
3. for i = 1:m
    1. 前向传播
    2. 计算损失函数
    3. 反向传播
4. 计算损失函数（该损失函数是多个样本损失函数的叠加）对权重的偏导（基于上述 m 个训练样本的）
5. 梯度检查。基于上述损失函数与权重的显性表达式，使用定义的方法计算损失函数对权重的偏导数。
6. 关闭梯度检查，仅使用反向传播算法，依据上述 m 个样本进行新的一轮又一轮的训练，直到满足训练目标。

# 神经网络的损失函数是非凸的
和逻辑回归、线性回归的损失函数是凸的不同，神经网络的损失函数是非凸的，也就是说，当使用不用的初始值的时候，神经网络可能收敛到不同的局部极小值处。这也就是当使用随机方法初始化权重时，往往得到不同的计算（优化）结果的原因。

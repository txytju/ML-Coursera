# 无监督学习：聚类算法
- 无监督学习
  - 聚类分析（Clustering）
    - 常用方法：K-Means


- K-Means 的步骤
  1. Input
      - 确认分几类（K类）
      - 待分类对象：m 个 n 维未标记数据
  2. 初始化K个初始分类中心（随机的）
  3. 循环，直到收敛
      1. 算所有的训练数据离哪个分类中心更近，就将这个数据归到这一类
      2. 重新计算K类的中心


- 将看起来分类不那么明显的数据用 K-Means 进行分类


- Optimization objective（K-Means 的损失函数）
  - $c^{(i)}$ 是当前训练样本 $x^{(i)}$ 所归属的类，$c^{(i)}$ 是类的编号（1...K）
  - $\mu_{k}$ 是第 $k$ 类的中心点位置
  - $\mu_{c^{(i)}}$ 是 $x^{(i)}$ 所属类的中心点
  - K-Means 的目标数让所有的点最终离他们的分类中点更近
$$J = \frac{1}{m} \sum ||x^{(i)} - \mu_{c^{(i)}}||^{2}$$

- 如何让损失函数变得更小
  - 循环中的两步都在让损失函数变得更小
    - 分类：将上一步不合理的分类分的更加合理，更加合理的分类会得到更小的损失函数
    - 求中点：只有点在已分类的所有点中间了，损失函数才会变小
  - 由上述可知，循环中的每一步都在让损失函数变得更小。因此在训练中，损失函数随迭代次数**严格地**越来越小才对。

- 随机初始化，局部最优
  - 随机初始化常用方法：随机选K个训练样本作为中心的初始化
  - 局部最优问题：随机初始化会有一定可能性导致落入局部最优。解决的方式是以不同的数值随机初始化很多次，然后选损失函数最小的一次对应的分类结果。这种方式在分类数K较小的时候（<10）管用，但是如果分类数量太多，即便随机初始化多次也可能不到全局最优。

- 选择聚类的类数（K）
  - 方法1：肘部法（不一定适用）。画损失函数值随着分类数（K）的变化曲线，选择变化明显的那个点，作为分类数。
  - 方法2：根据下游的业务选。根据下游实际需要几类，确定聚类的数量。



# Principal Components Analysis (PCA, 主成分分析)

- 无监督学习方法：维度约减（`Dimensionality Reduction`）
- 为什么要进行维度约减
  - 为了压缩数据
    - 压缩数据的好处
      - 节省存储的硬盘内存
      - 在训练某些算法时速度提升
  - 数据可视化
    - 将高维数据降到2维或者3维，以进行可视化

- 常用的数据降维方法：PCA
  - 通俗解释：将原有的高维数据投影到低维度的平面或者超平面上，并且使原始数据点到这个超平面的距离的平方和最小。即，找一个合适的超平面，将高维数据投影上去。
  - 数学解释：假设原有的数据空间是n维，想要将数据降到k维，那么方法就是在原有的n维空间中找到k个向量，由这k个向量组成的k维子空间作为被投影对象，将原有的n维数据投影过去，并设法找到距离平方和最小的k维子空间。
  - 在进行PCA之前，首先要对数据进行预处理，包括特征缩放和均值归一化。

- 如何使用PCA加快算法的运算速度
  - 在什么算法中使用PCA：通常在有监督学习中使用PCA降低X的维度（比如图像识别领域）。
  - 如何使用PCA
    1. 在训练样本的 X 上应用PCA，并得到由高维空间向低维空间的映射矩阵 $U_{reduce}$。上述映射矩阵是 PCA 这种无监督学习方法的训练结果。
    2. 使用 $U_{reduce}$，将训练样本 X 映射到低维度空间中的 Z，并以 Z 为输入元素训练有监督学习模型。至此，有监督学习的模型已经完整训练完毕。
    3. 如果希望使用测试集测试有监督学习模型的效果，应使用上述 $U_{reduce}$ 将 $X_{test}$ 映射到低维空间中，并使用有监督学习算法进行预测或者分类，并验证模型的效果。
    4. 在上述过程中需要注意的一点是，$U_{reduce}$ 的训练过程，即PCA无监督学习的训练过程中只能使用训练集中的 X。
- 数据降维对模型的训练效果影响大吗？
  - 在视频中提到了一个计算机视觉的例子，将一个10000维的图片数据（100*100像素）降维到原有维度的十分之一，即1000维，并以此训练分类模型，对模型的效果几乎没有影响。
  - **测试：寻找100*100维度的数据，并使用PCA将其降到1000维（K=1000），查看其保留了原有的信息的比例？**
- 不要使用 PCA 做哪些事？
  - 不要使用 PCA 来降低过拟合
    - 因为在使用 PCA 算法的过程中，算法只能拿到 X 的信息，不能拿到 y 的信息（把 y 的信息扔掉了）。过拟合还是使用正则化来避免。
  - 不要滥用 PCA
    - 虽然 PCA 是一种非常有效的无监督学习算法，但是并不是所有情况下都需要使用 PCA 的。当刚拿到一个问题及数据时，首先考虑的应该是不使用 PCA，而是在原始数据上进行模型的训练。只有当有足够的证据证明必须使用 PCA 时（如模型的训练速度非常非常慢，或者需要的内存或者硬盘超过了可接受的范围），才使用 PCA。
